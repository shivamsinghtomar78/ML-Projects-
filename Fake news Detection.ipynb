{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNtAbXUPmPkxnkR3i0sxYYY"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["About the DataSet:\n","\n","1.Id:Unique id for a news article\n","\n","2.title:the title of a news article\n","\n","3.author: author of the news article\n","\n","4.text: the text of the article ;could be compltete\n","\n","5.label: a label that marks whether the news article is real or fake"],"metadata":{"id":"bIcnvelrEVKl"}},{"cell_type":"markdown","source":["     1: Fake news\n","\n","      0: real news"],"metadata":{"id":"VRICwjSCGIKF"}},{"cell_type":"code","execution_count":16,"metadata":{"id":"augnqolcDbYX","executionInfo":{"status":"ok","timestamp":1737703278539,"user_tz":-330,"elapsed":368,"user":{"displayName":"Shivam singh","userId":"15204433894062641447"}}},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import re\n","import sklearn\n","import pickle\n","from nltk.corpus import stopwords\n","from nltk.stem.porter import PorterStemmer\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import accuracy_score\n","import nltk"]},{"cell_type":"code","source":["nltk.download('stopwords')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yucyAJvWKyrY","executionInfo":{"status":"ok","timestamp":1737703281438,"user_tz":-330,"elapsed":10,"user":{"displayName":"Shivam singh","userId":"15204433894062641447"}},"outputId":"7057c98f-7d2d-4f9c-9855-afc0e9dabbb7"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":17}]},{"cell_type":"code","source":["#printing the stopwords in English\n","print(stopwords.words('english'))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Bz0j5kLXLRkl","executionInfo":{"status":"ok","timestamp":1737703284345,"user_tz":-330,"elapsed":5,"user":{"displayName":"Shivam singh","userId":"15204433894062641447"}},"outputId":"24d93885-6a06-485e-9f93-707f0926dcaf"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"]}]},{"cell_type":"markdown","source":["Data Pre-Processing"],"metadata":{"id":"br8VnuSSL9vf"}},{"cell_type":"code","source":["#Loading the dataset to a pandas Dataframes\n","news_dataset = pd.read_csv('/content/train.csv')"],"metadata":{"id":"cbAIUCYPLY4m","executionInfo":{"status":"error","timestamp":1737703475987,"user_tz":-330,"elapsed":414,"user":{"displayName":"Shivam singh","userId":"15204433894062641447"}},"colab":{"base_uri":"https://localhost:8080/","height":495},"outputId":"c7692c01-b16d-4e9a-d1e6-d94dae357277","collapsed":true},"execution_count":27,"outputs":[{"output_type":"error","ename":"ParserError","evalue":"Error tokenizing data. C error: Expected 5 fields in line 196, saw 7\n","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m<ipython-input-27-64203361a42e>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Loading the dataset to a pandas Dataframes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnews_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/train.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    624\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 626\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1921\u001b[0m                     \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1922\u001b[0m                     \u001b[0mcol_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1923\u001b[0;31m                 \u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1924\u001b[0m                     \u001b[0mnrows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1925\u001b[0m                 )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlow_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m                 \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_low_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m                 \u001b[0;31m# destructive to chunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_concatenate_chunks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n","\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n","\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n","\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[0;34m()\u001b[0m\n","\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n","\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: Expected 5 fields in line 196, saw 7\n"]}]},{"cell_type":"code","source":["news_dataset.shape"],"metadata":{"id":"6r6pxgFkMNUI","executionInfo":{"status":"error","timestamp":1737703447918,"user_tz":-330,"elapsed":404,"user":{"displayName":"Shivam singh","userId":"15204433894062641447"}},"colab":{"base_uri":"https://localhost:8080/","height":141},"outputId":"8ac7ecf8-33f7-4e8d-f1eb-1857f8575b6f"},"execution_count":26,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'news_dataset' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-26-45c5752a66f1>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnews_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'news_dataset' is not defined"]}]},{"cell_type":"code","source":["news_dataset.head()"],"metadata":{"id":"Ud9AZr_xMvKw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#counting the number of missing values in the dataset\n","print(\"\\nMissing values:\")\n","print(news_dataset.isnull().sum())"],"metadata":{"id":"SDoxDn91NF1b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#replacing the null values with empty string\n","news_dataset = news_dataset.fillna('')"],"metadata":{"id":"ymA8uEedNWdv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Merging the author name and news title\n","news_dataset['content'] = news_dataset['author'] + ' ' + news_dataset['title']\n","print(\"\\nCombined content:\")\n","print(news_dataset['content'])"],"metadata":{"id":"0inQl4p4OXkN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#separating the data and label\n","X = news_dataset.drop(columns='label', axis=1)\n","Y = news_dataset['label']\n","print(\"\\nFeatures:\")\n","print(X)\n","print(\"\\nLabels:\")\n","print(Y)"],"metadata":{"id":"0S2RE7m2PZ-N"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Stemming:\n","\n","Stemming is the process of reducing a word to its root word\n","\n","example:\n","\n","actor,actress,acting-->act"],"metadata":{"id":"9I67T-L2Qh-Y"}},{"cell_type":"code","source":["port_stem = PorterStemmer()\n","\n","def stemming(content):\n","    stemmed_content = re.sub('[^a-zA-Z]', ' ', content)\n","    stemmed_content = stemmed_content.lower()\n","    stemmed_content = stemmed_content.split()\n","    stemmed_content = [port_stem.stem(word) for word in stemmed_content if not word in stopwords.words('english')]\n","    stemmed_content = ' '.join(stemmed_content)\n","    return stemmed_content"],"metadata":{"id":"DhPtsEh2P9Ph"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Apply stemming to content\n","news_dataset['content'] = news_dataset['content'].apply(stemming)\n","print(\"\\nProcessed content:\")\n","print(news_dataset['content'])"],"metadata":{"id":"8l6qPTBMWmaK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Separating the data and label\n","X = news_dataset['content'].values\n","Y = news_dataset['label'].values\n","print(\"\\nFeature array:\")\n","print(X)\n","print(\"\\nLabel array:\")\n","print(Y)\n","print(\"\\nLabel shape:\", Y.shape)"],"metadata":{"id":"4oHlJyyBZz8z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Converting the textual data to numerical data\n","vectorizer = TfidfVectorizer()\n","vectorizer.fit(X)\n","X = vectorizer.transform(X)\n","print(\"\\nVectorized features:\")\n","print(X)"],"metadata":{"id":"sFEXN7QkbHYS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Spliting the dataSet to trainig and test data"],"metadata":{"id":"WcLOq3JRdVul"}},{"cell_type":"code","source":["X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, stratify=Y, random_state=2)"],"metadata":{"id":"fG388Cn4cyyx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Training the model Logistic Regression"],"metadata":{"id":"WFgemkncfjl3"}},{"cell_type":"code","source":["model=LogisticRegression();"],"metadata":{"id":"P9hef8O0dmZb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.fit(X_train,Y_train)"],"metadata":{"id":"RzFBQ5fffwIK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Evaluation"],"metadata":{"id":"upvyVC4Xh1Nv"}},{"cell_type":"markdown","source":[" Accuracy Score"],"metadata":{"id":"smokHwCGh4HS"}},{"cell_type":"code","source":["# Accuracy score on the training data\n","X_train_prediction = model.predict(X_train)\n","training_data_accuracy = accuracy_score(X_train_prediction, Y_train)\n","print('\\nAccuracy score of the training data:', training_data_accuracy)"],"metadata":{"id":"2UuNst7ahxX0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Accuracy score on the test data\n","X_test_prediction = model.predict(X_test)\n","test_data_accuracy = accuracy_score(X_test_prediction, Y_test)\n","print('Accuracy score of the test data:', test_data_accuracy)"],"metadata":{"id":"kMc4YcV_iznd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Making a Predictive System"],"metadata":{"id":"EJI7KbfUkBl_"}},{"cell_type":"code","source":["X_new = X_test[0]\n","prediction = model.predict(X_new)\n","if prediction[0] == 0:\n","    print(\"\\nThe news is Real\")\n","else:\n","    print(\"\\nThe news is Fake\")"],"metadata":{"id":"T5HUHkAOjjLl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Print version information\n","print(\"\\nNumpy version:\", np.__version__)\n","print(\"Pandas version:\", pd.__version__)\n","print(\"Scikit-learn version:\", sklearn.__version__)"],"metadata":{"id":"S1bODx5kstY4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Saving the trained model and vectorizer together"],"metadata":{"id":"jbKgvy6RnvQM"}},{"cell_type":"code","source":["model_data = {\n","    'model': model,\n","    'vectorizer': vectorizer\n","}\n","with open('trained_model.pkl', 'wb') as file:\n","    pickle.dump(model_data, file)\n","\n","print(\"\\nModel and vectorizer saved successfully!\")"],"metadata":{"id":"WRxJqUJakNGi"},"execution_count":null,"outputs":[]}]}